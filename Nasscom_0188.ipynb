{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Nasscom_0188.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kr1gsWdAZp-I",
        "outputId": "b705fa2e-851e-4249-e92a-375a60520f55"
      },
      "source": [
        "#Shreya Balachandra Naik\n",
        "#5COM 4 \n",
        "#20191COM0188\n",
        "\n",
        "# The correct values of the variables m and b can be actually calculated easily rather than doing linear regression.\n",
        "# But to demonstrate the gradient descent this code has been writen \n",
        "\n",
        "from numpy import *\n",
        "\n",
        "\n",
        "# y = mx + c\n",
        "# m is slope, c is y-intercept\n",
        "def compute_error_for_line_given_points(b, m, points):\n",
        "    total_error = 0\n",
        "    for i in range(0, len(points)):\n",
        "        x = points[i, 0]\n",
        "        y = points[i, 1]\n",
        "        total_error += (y - (m * x + b)) ** 2\n",
        "    return total_error / float(len(points))\n",
        "\n",
        "def gradient_descent_runner(points, starting_b, starting_m, learning_rate, num_iterations):\n",
        "    b = starting_b\n",
        "    m = starting_m\n",
        "    for i in range(num_iterations):\n",
        "        b, m = step_gradient(b, m, array(points), learning_rate)\n",
        "    return [b, m]\n",
        "\n",
        "def step_gradient(b_current, m_current, points, learningRate):\n",
        "    b_gradient = 0\n",
        "    m_gradient = 0\n",
        "    N = float(len(points))\n",
        "    for i in range(0, len(points)):\n",
        "        x = points[i, 0]\n",
        "        y = points[i, 1]\n",
        "        b_gradient += -(2 / N) * (y - ((m_current * x) + b_current))\n",
        "        m_gradient += -(2 / N) * x * (y - ((m_current * x) + b_current))\n",
        "    new_b = b_current - (learningRate * b_gradient)\n",
        "    new_m = m_current - (learningRate * m_gradient)\n",
        "    return [new_b, new_m]\n",
        "\n",
        "\n",
        "def run():\n",
        "    points = genfromtxt(\"/content/data0188.csv\", delimiter=\",\")\n",
        "    learning_rate = 0.0001\n",
        "    ini_b = 0  # initial y-intercept guess\n",
        "    ini_m = 0  # initial slope guess\n",
        "    num_iterations = 1000\n",
        "    print(\"Starting gradient descent at b = {0}, m = {1}, error = {2}\".format(ini_b, ini_m,\n",
        "                                                                              compute_error_for_line_given_points(\n",
        "                                                                                  ini_b, ini_m, points)))\n",
        "    print(\"Running...\")\n",
        "    [b, m] = gradient_descent_runner(points, ini_b, ini_m, learning_rate, num_iterations)\n",
        "    print(\"After {0} iterations b = {1}, m = {2}, error = {3}\".format(num_iterations, b, m,\n",
        "                                                                      compute_error_for_line_given_points(b, m,\n",
        "                                                                                                          points)))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run()\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting gradient descent at b = 0, m = 0, error = 5565.107834483211\n",
            "Running...\n",
            "After 1000 iterations b = 0.08893651993741346, m = 1.4777440851894448, error = 112.61481011613473\n"
          ]
        }
      ]
    }
  ]
}